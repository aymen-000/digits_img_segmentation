{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ3g9dJxSxmN"
      },
      "source": [
        "## Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aifz2907kxYN"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RYh6cCzXE6R"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUGGF3wfqYni"
      },
      "source": [
        "[M2NIST](https://www.kaggle.com/farhanhubble/multimnistm2nist) is a **multi digit** [MNIST](http://yann.lecun.com/exdb/mnist/).\n",
        "Each image has up to 3 digits from MNIST digits and the corresponding labels file has the segmentation masks.\n",
        "\n",
        "The dataset is available on [Kaggle](https://www.kaggle.com) and you can find it [here](https://www.kaggle.com/farhanhubble/multimnistm2nist)\n",
        "\n",
        "To make it easier for you, we're hosting it on Google Cloud so you can download without Kaggle credentials.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROok0i9rMcu0"
      },
      "outputs": [],
      "source": [
        "# download zipped dataset\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/m2nist.zip \\\n",
        "    -O /tmp/m2nist.zip\n",
        "\n",
        "# find and extract to a local folder ('/tmp/training')\n",
        "local_zip = '/tmp/m2nist.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/training')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy17LYR7XJNa"
      },
      "source": [
        "## Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy_pw5I2-xLP"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "def read_image_and_annotation(image, annotation):\n",
        "  '''\n",
        "  Casts the image and annotation to their expected data type and\n",
        "  normalizes the input image so that each pixel is in the range [-1, 1]\n",
        "\n",
        "  Args:\n",
        "    image (numpy array) -- input image\n",
        "    annotation (numpy array) -- ground truth label map\n",
        "\n",
        "  Returns:\n",
        "    preprocessed image-annotation pair\n",
        "  '''\n",
        "\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = tf.reshape(image, (image.shape[0], image.shape[1], 1,))\n",
        "  annotation = tf.cast(annotation, dtype=tf.int32)\n",
        "  image = image / 127.5\n",
        "  image -= 1\n",
        "\n",
        "  return image, annotation\n",
        "\n",
        "\n",
        "def get_training_dataset(images, annos):\n",
        "  '''\n",
        "  Prepares shuffled batches of the training set.\n",
        "\n",
        "  Args:\n",
        "    images (list of strings) -- paths to each image file in the train set\n",
        "    annos (list of strings) -- paths to each label map in the train set\n",
        "\n",
        "  Returns:\n",
        "    tf Dataset containing the preprocessed train set\n",
        "  '''\n",
        "  training_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  training_dataset = training_dataset.map(read_image_and_annotation)\n",
        "\n",
        "  training_dataset = training_dataset.shuffle(512, reshuffle_each_iteration=True)\n",
        "  training_dataset = training_dataset.batch(BATCH_SIZE)\n",
        "  training_dataset = training_dataset.repeat()\n",
        "  training_dataset = training_dataset.prefetch(-1)\n",
        "\n",
        "  return training_dataset\n",
        "\n",
        "\n",
        "def get_validation_dataset(images, annos):\n",
        "  '''\n",
        "  Prepares batches of the validation set.\n",
        "\n",
        "  Args:\n",
        "    images (list of strings) -- paths to each image file in the val set\n",
        "    annos (list of strings) -- paths to each label map in the val set\n",
        "\n",
        "  Returns:\n",
        "    tf Dataset containing the preprocessed validation set\n",
        "  '''\n",
        "  validation_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  validation_dataset = validation_dataset.map(read_image_and_annotation)\n",
        "  validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
        "  validation_dataset = validation_dataset.repeat()\n",
        "\n",
        "  return validation_dataset\n",
        "\n",
        "\n",
        "def get_test_dataset(images, annos):\n",
        "  '''\n",
        "  Prepares batches of the test set.\n",
        "\n",
        "  Args:\n",
        "    images (list of strings) -- paths to each image file in the test set\n",
        "    annos (list of strings) -- paths to each label map in the test set\n",
        "\n",
        "  Returns:\n",
        "    tf Dataset containing the preprocessed validation set\n",
        "  '''\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  test_dataset = test_dataset.map(read_image_and_annotation)\n",
        "  test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  return test_dataset\n",
        "\n",
        "\n",
        "def load_images_and_segments():\n",
        "  '''\n",
        "  Loads the images and segments as numpy arrays from npy files\n",
        "  and makes splits for training, validation and test datasets.\n",
        "\n",
        "  Returns:\n",
        "    3 tuples containing the train, val, and test splits\n",
        "  '''\n",
        "\n",
        "  #Loads images and segmentation masks.\n",
        "  images = np.load('/tmp/training/combined.npy')\n",
        "  segments = np.load('/tmp/training/segmented.npy')\n",
        "\n",
        "  #Makes training, validation, test splits from loaded images and segmentation masks.\n",
        "  train_images, val_images, train_annos, val_annos = train_test_split(images, segments, test_size=0.2, shuffle=True)\n",
        "  val_images, test_images, val_annos, test_annos = train_test_split(val_images, val_annos, test_size=0.2, shuffle=True)\n",
        "\n",
        "  return (train_images, train_annos), (val_images, val_annos), (test_images, test_annos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPHO1YbTACcu"
      },
      "source": [
        "You can now load the preprocessed dataset and define the training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIS70_um_Y7n",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "train_slices, val_slices, test_slices = load_images_and_segments()\n",
        "\n",
        "# Create training, validation, test datasets.\n",
        "training_dataset = get_training_dataset(train_slices[0], train_slices[1])\n",
        "validation_dataset = get_validation_dataset(val_slices[0], val_slices[1])\n",
        "test_dataset = get_test_dataset(test_slices[0], test_slices[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKXJYZi7A0dF"
      },
      "source": [
        "## Let's Take a Look at the Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "d46YCbvPafbp"
      },
      "outputs": [],
      "source": [
        "# Visualization Utilities\n",
        "\n",
        "# there are 11 classes in the dataset: one class for each digit (0 to 9) plus the background class\n",
        "n_classes = 11\n",
        "\n",
        "# assign a random color for each class\n",
        "colors = [tuple(np.random.randint(256, size=3) / 255.0) for i in range(n_classes)]\n",
        "\n",
        "def fuse_with_pil(images):\n",
        "  '''\n",
        "  Creates a blank image and pastes input images\n",
        "\n",
        "  Args:\n",
        "    images (list of numpy arrays) - numpy array representations of the images to paste\n",
        "\n",
        "  Returns:\n",
        "    PIL Image object containing the images\n",
        "  '''\n",
        "\n",
        "  widths = (image.shape[1] for image in images)\n",
        "  heights = (image.shape[0] for image in images)\n",
        "  total_width = sum(widths)\n",
        "  max_height = max(heights)\n",
        "\n",
        "  new_im = PIL.Image.new('RGB', (total_width, max_height))\n",
        "\n",
        "  x_offset = 0\n",
        "  for im in images:\n",
        "    pil_image = PIL.Image.fromarray(np.uint8(im))\n",
        "    new_im.paste(pil_image, (x_offset,0))\n",
        "    x_offset += im.shape[1]\n",
        "\n",
        "  return new_im\n",
        "\n",
        "\n",
        "def give_color_to_annotation(annotation):\n",
        "  '''\n",
        "  Converts a 2-D annotation to a numpy array with shape (height, width, 3) where\n",
        "  the third axis represents the color channel. The label values are multiplied by\n",
        "  255 and placed in this axis to give color to the annotation\n",
        "\n",
        "  Args:\n",
        "    annotation (numpy array) - label map array\n",
        "\n",
        "  Returns:\n",
        "    the annotation array with an additional color channel/axis\n",
        "  '''\n",
        "  seg_img = np.zeros( (annotation.shape[0],annotation.shape[1], 3) ).astype('float')\n",
        "\n",
        "  for c in range(n_classes):\n",
        "    segc = (annotation == c)\n",
        "    seg_img[:,:,0] += segc*( colors[c][0] * 255.0)\n",
        "    seg_img[:,:,1] += segc*( colors[c][1] * 255.0)\n",
        "    seg_img[:,:,2] += segc*( colors[c][2] * 255.0)\n",
        "\n",
        "  return seg_img\n",
        "\n",
        "\n",
        "def show_annotation_and_prediction(image, annotation, prediction, iou_list, dice_score_list):\n",
        "  '''\n",
        "  Displays the images with the ground truth and predicted label maps. Also overlays the metrics.\n",
        "\n",
        "  Args:\n",
        "    image (numpy array) -- the input image\n",
        "    annotation (numpy array) -- the ground truth label map\n",
        "    prediction (numpy array) -- the predicted label map\n",
        "    iou_list (list of floats) -- the IOU values for each class\n",
        "    dice_score_list (list of floats) -- the Dice Score for each class\n",
        "  '''\n",
        "\n",
        "  new_ann = np.argmax(annotation, axis=2)\n",
        "  true_img = give_color_to_annotation(new_ann)\n",
        "  pred_img = give_color_to_annotation(prediction)\n",
        "\n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  image = np.reshape(image, (image.shape[0], image.shape[1],))\n",
        "  image = np.uint8(image)\n",
        "  images = [image, np.uint8(pred_img), np.uint8(true_img)]\n",
        "\n",
        "  metrics_by_id = [(idx, iou, dice_score) for idx, (iou, dice_score) in enumerate(zip(iou_list, dice_score_list)) if iou > 0.0 and idx < 10]\n",
        "  metrics_by_id.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place\n",
        "\n",
        "  display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(idx, iou, dice_score) for idx, iou, dice_score in metrics_by_id]\n",
        "  display_string = \"\\n\".join(display_string_list)\n",
        "\n",
        "  plt.figure(figsize=(15, 4))\n",
        "\n",
        "  for idx, im in enumerate(images):\n",
        "    plt.subplot(1, 3, idx+1)\n",
        "    if idx == 1:\n",
        "      plt.xlabel(display_string)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(im)\n",
        "\n",
        "\n",
        "def show_annotation_and_image(image, annotation):\n",
        "  '''\n",
        "  Displays the image and its annotation side by side\n",
        "\n",
        "  Args:\n",
        "    image (numpy array) -- the input image\n",
        "    annotation (numpy array) -- the label map\n",
        "  '''\n",
        "  new_ann = np.argmax(annotation, axis=2)\n",
        "  seg_img = give_color_to_annotation(new_ann)\n",
        "\n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  image = np.reshape(image, (image.shape[0], image.shape[1],))\n",
        "\n",
        "  image = np.uint8(image)\n",
        "  images = [image, seg_img]\n",
        "\n",
        "  images = [image, seg_img]\n",
        "  fused_img = fuse_with_pil(images)\n",
        "  plt.imshow(fused_img)\n",
        "\n",
        "\n",
        "def list_show_annotation(dataset, num_images):\n",
        "  '''\n",
        "  Displays images and its annotations side by side\n",
        "\n",
        "  Args:\n",
        "    dataset (tf Dataset) -- batch of images and annotations\n",
        "    num_images (int) -- number of images to display\n",
        "  '''\n",
        "  ds = dataset.unbatch()\n",
        "\n",
        "  plt.figure(figsize=(20, 15))\n",
        "  plt.title(\"Images And Annotations\")\n",
        "  plt.subplots_adjust(bottom=0.1, top=0.9, hspace=0.05)\n",
        "\n",
        "  for idx, (image, annotation) in enumerate(ds.take(num_images)):\n",
        "    plt.subplot(5, 5, idx + 1)\n",
        "    plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    show_annotation_and_image(image.numpy(), annotation.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEfyChmKEFKe"
      },
      "source": [
        "You can view a subset of the images from the dataset with the `list_show_annotation()` function defined above. Run the cells below to see the image on the left and its pixel-wise ground truth label map on the right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFO_hIhLWYT4"
      },
      "outputs": [],
      "source": [
        "# get 10 images from the training set\n",
        "list_show_annotation(training_dataset, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdgVkp8wZua0"
      },
      "outputs": [],
      "source": [
        "# get 10 images from the validation set\n",
        "list_show_annotation(validation_dataset, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFv2k8xabRb8"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHlBUZvsDybt"
      },
      "source": [
        "### Define the Basic Convolution Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azEEVytHR0Kn"
      },
      "outputs": [],
      "source": [
        "# parameter describing where the channel dimension is found in our dataset\n",
        "IMAGE_ORDERING = 'channels_last'\n",
        "\n",
        "def conv_block(input, filters, kernel_size, pooling_size, pool_strides):\n",
        "  '''\n",
        "  Args:\n",
        "    input (tensor) -- batch of images or features\n",
        "    filters (int) -- number of filters of the Conv2D layers\n",
        "    kernel_size (int) -- kernel_size setting of the Conv2D layers\n",
        "    pooling_size (int) -- pooling size of the MaxPooling2D layers\n",
        "    pool_strides (int) -- strides setting of the MaxPooling2D layers\n",
        "\n",
        "  Returns:\n",
        "    (tensor) max pooled and batch-normalized features of the input\n",
        "  '''\n",
        "  ### START CODE HERE ###\n",
        "  # use the functional syntax to stack the layers as shown in the diagram above\n",
        "  x = tf.keras.layers.Conv2D(filters, kernel_size, padding='same', data_format=IMAGE_ORDERING)(input)\n",
        "  x = tf.keras.layers.LeakyReLU()(x)\n",
        "  x = tf.keras.layers.Conv2D(filters, kernel_size, padding='same', data_format=IMAGE_ORDERING)(x)\n",
        "  x = tf.keras.layers.LeakyReLU()(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=pooling_size, strides=pool_strides)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGtlHuM6KCRg"
      },
      "outputs": [],
      "source": [
        "# TEST CODE:\n",
        "\n",
        "test_input = tf.keras.layers.Input(shape=(64,84, 1))\n",
        "test_output = conv_block(test_input, 32, 3, 2, 2)\n",
        "test_model = tf.keras.Model(inputs=test_input, outputs=test_output)\n",
        "\n",
        "print(test_model.summary())\n",
        "\n",
        "# free up test resources\n",
        "del test_input, test_output, test_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-jJbC91EXTV"
      },
      "source": [
        "### Define the Downsampling Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2VNB99LRwQr"
      },
      "outputs": [],
      "source": [
        "def FCN8(input_height=64, input_width=84):\n",
        "    '''\n",
        "    Defines the downsampling path of the image segmentation model.\n",
        "\n",
        "    Args:\n",
        "      input_height (int) -- height of the images\n",
        "      width (int) -- width of the images\n",
        "\n",
        "    Returns:\n",
        "    (tuple of tensors, tensor)\n",
        "      tuple of tensors -- features extracted at blocks 3 to 5\n",
        "      tensor -- copy of the input\n",
        "    '''\n",
        "\n",
        "    img_input = tf.keras.layers.Input(shape=(input_height,input_width, 1))\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # pad the input image width to 96 pixels\n",
        "    x = tf.keras.layers.ZeroPadding2D(((0, 0), (0, 96-input_width)))(img_input)\n",
        "\n",
        "    # Block 1\n",
        "    x = conv_block(x, filters=32, kernel_size=(3,3), pooling_size=(2,2), pool_strides=(2,2))\n",
        "\n",
        "    # Block 2\n",
        "    x =  conv_block(x, filters=64, kernel_size=(3,3), pooling_size=(2,2), pool_strides=(2,2))\n",
        "\n",
        "    # Block 3\n",
        "    x =conv_block(x, filters=128, kernel_size=(3,3), pooling_size=(2,2), pool_strides=(2,2))\n",
        "    # save the feature map at this stage\n",
        "    f3 = x\n",
        "\n",
        "    # Block 4\n",
        "    x  = conv_block(x, filters=256, kernel_size=(3,3), pooling_size=(2,2), pool_strides=(2,2))\n",
        "    # save the feature map at this stage\n",
        "    f4 =x\n",
        "\n",
        "    # Block 5\n",
        "    x = conv_block(x, filters=256, kernel_size=(3,3), pooling_size=(2,2), pool_strides=(2,2))\n",
        "    # save the feature map at this stage\n",
        "    f5 = x\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return (f3, f4, f5), img_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVQm1W0CNICS"
      },
      "outputs": [],
      "source": [
        "# TEST CODE:\n",
        "\n",
        "test_convs, test_img_input = FCN8()\n",
        "test_model = tf.keras.Model(inputs=test_img_input, outputs=[test_convs, test_img_input])\n",
        "\n",
        "print(test_model.summary())\n",
        "\n",
        "del test_convs, test_img_input, test_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbjYEQU8Eq-T"
      },
      "source": [
        "### Define the FCN-8 decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giYEct_Se5Xj"
      },
      "outputs": [],
      "source": [
        "def fcn8_decoder(convs, n_classes):\n",
        "    # features from the encoder stage\n",
        "    f3, f4, f5 = convs\n",
        "\n",
        "    # number of filters\n",
        "    n = 512\n",
        "\n",
        "    # add convolutional layers on top of the CNN extractor.\n",
        "    o = tf.keras.layers.Conv2D(n, (7, 7), activation='relu', padding='same', name=\"conv6\", data_format=IMAGE_ORDERING)(f5)\n",
        "    o = tf.keras.layers.Dropout(0.5)(o)\n",
        "\n",
        "    o = tf.keras.layers.Conv2D(n, (1, 1), activation='relu', padding='same', name=\"conv7\", data_format=IMAGE_ORDERING)(o)\n",
        "    o = tf.keras.layers.Dropout(0.5)(o)\n",
        "\n",
        "    o = tf.keras.layers.Conv2D(n_classes, (1, 1), activation='relu', padding='same', data_format=IMAGE_ORDERING)(o)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Upsample `o` above and crop any extra pixels introduced\n",
        "    o = tf.keras.layers.Conv2DTranspose(11, (4, 4), strides=(2, 2), use_bias=False)(o)\n",
        "    o = tf.keras.layers.Cropping2D(cropping=(1, 1))(o)\n",
        "\n",
        "    # load the pool 4 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
        "    o2 = f4\n",
        "    o2 = tf.keras.layers.Conv2D(11, (1, 1), activation='relu', padding='same')(o2)\n",
        "\n",
        "    # add the results of the upsampling and pool 4 prediction\n",
        "    o = tf.keras.layers.add([o, o2])\n",
        "\n",
        "    # upsample the resulting tensor of the operation you just did\n",
        "    o = tf.keras.layers.Conv2DTranspose(n_classes, kernel_size=(4, 4), strides=(2, 2), use_bias=False)(o)\n",
        "    o = tf.keras.layers.Cropping2D(cropping=(1, 1))(o)\n",
        "\n",
        "    # load the pool 3 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
        "    o2 = f3\n",
        "    o2 = tf.keras.layers.Conv2D(n_classes, (1, 1), activation='relu', padding='same', data_format=IMAGE_ORDERING)(o2)\n",
        "\n",
        "    # add the results of the upsampling and pool 3 prediction\n",
        "    o = tf.keras.layers.add([o, o2])\n",
        "\n",
        "    # upsample up to the size of the original image\n",
        "    o = tf.keras.layers.Conv2DTranspose(n_classes, kernel_size=(8, 8), strides=(8, 8), use_bias=False)(o)\n",
        "    o = tf.keras.layers.Cropping2D(((0, 0), (0, 96 - 84)))(o)\n",
        "\n",
        "    # append a sigmoid activation\n",
        "    o = (tf.keras.layers.Activation('sigmoid'))(o)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQHioDlR5K1_"
      },
      "outputs": [],
      "source": [
        "# TEST CODE\n",
        "\n",
        "test_convs, test_img_input = FCN8()\n",
        "test_fcn8_decoder = fcn8_decoder(test_convs, 11)\n",
        "\n",
        "print(test_fcn8_decoder.shape)\n",
        "\n",
        "del test_convs, test_img_input, test_fcn8_decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJOhQz86Qk6n"
      },
      "source": [
        "### Define the Complete Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EJEf484312h",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# start the encoder using the default input size 64 x 84\n",
        "convs, img_input = FCN8()\n",
        "\n",
        "# pass the convolutions obtained in the encoder to the decoder\n",
        "dec_op = fcn8_decoder(convs, n_classes)\n",
        "\n",
        "# define the model specifying the input (batch of images) and output (decoder output)\n",
        "model = tf.keras.Model(inputs = img_input, outputs = dec_op)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GAenp1M4gXx"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAAXygZtbZmu"
      },
      "source": [
        "## Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpWpp8h4g_rE"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "model.compile(loss=tf.keras.losses.CategoricalFocalCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510v0aVDXv1f"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HoZwpGWhMB-"
      },
      "outputs": [],
      "source": [
        "# OTHER THAN SETTING THE EPOCHS NUMBER, DO NOT CHANGE ANY OTHER CODE\n",
        "\n",
        "### START CODE HERE ###\n",
        "EPOCHS = 10\n",
        "### END CODE HERE ###\n",
        "\n",
        "steps_per_epoch = 4000//BATCH_SIZE\n",
        "validation_steps = 800//BATCH_SIZE\n",
        "test_steps = 200//BATCH_SIZE\n",
        "\n",
        "\n",
        "history = model.fit(training_dataset,\n",
        "                    steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eih-Q7GoXzJe"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zENjQuK0luH5"
      },
      "outputs": [],
      "source": [
        "results = model.predict(test_dataset, steps=test_steps)\n",
        "\n",
        "print(results.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwFiR9WAf0Av"
      },
      "outputs": [],
      "source": [
        "print(results[0,0,0,0])\n",
        "print(results[0,0,0,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_Uj_uuV9TQt"
      },
      "outputs": [],
      "source": [
        "results = np.argmax(results, axis=3)\n",
        "\n",
        "print(results.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBeBwvHQd2pZ"
      },
      "outputs": [],
      "source": [
        "print(results[0,0,0])\n",
        "\n",
        "# prediction map for image 0\n",
        "print(results[0,:,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKTpLmly_RXb"
      },
      "outputs": [],
      "source": [
        "def class_wise_metrics(y_true, y_pred):\n",
        "  '''\n",
        "  Computes the class-wise IOU and Dice Score.\n",
        "\n",
        "  Args:\n",
        "    y_true (tensor) - ground truth label maps\n",
        "    y_pred (tensor) - predicted label maps\n",
        "  '''\n",
        "  class_wise_iou = []\n",
        "  class_wise_dice_score = []\n",
        "\n",
        "  smoothing_factor = 0.00001\n",
        "\n",
        "  for i in range(n_classes):\n",
        "    intersection = np.sum((y_pred == i) * (y_true == i))\n",
        "    y_true_area = np.sum((y_true == i))\n",
        "    y_pred_area = np.sum((y_pred == i))\n",
        "    combined_area = y_true_area + y_pred_area\n",
        "\n",
        "    iou = (intersection) / (combined_area - intersection + smoothing_factor)\n",
        "    class_wise_iou.append(iou)\n",
        "\n",
        "    dice_score =  2 * ((intersection) / (combined_area + smoothing_factor))\n",
        "    class_wise_dice_score.append(dice_score)\n",
        "\n",
        "  return class_wise_iou, class_wise_dice_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfWPwM4ZhHjE"
      },
      "source": [
        "### Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkbsk_P1fpRM",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# place a number here between 0 to 191 to pick an image from the test set\n",
        "integer_slider = 105\n",
        "\n",
        "ds = test_dataset.unbatch()\n",
        "ds = ds.batch(200)\n",
        "images = []\n",
        "\n",
        "y_true_segments = []\n",
        "for image, annotation in ds.take(2):\n",
        "  y_true_segments = annotation\n",
        "  images = image\n",
        "\n",
        "\n",
        "iou, dice_score = class_wise_metrics(np.argmax(y_true_segments[integer_slider], axis=2), results[integer_slider])\n",
        "show_annotation_and_prediction(image[integer_slider], annotation[integer_slider], results[integer_slider], iou, dice_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiG9K4t6X9iZ"
      },
      "source": [
        "### Compute IOU Score and Dice Score of your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2706boF0CNNS",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "cls_wise_iou, cls_wise_dice_score = class_wise_metrics(np.argmax(y_true_segments, axis=3), results)\n",
        "\n",
        "average_iou = 0.0\n",
        "for idx, (iou, dice_score) in enumerate(zip(cls_wise_iou[:-1], cls_wise_dice_score[:-1])):\n",
        "  print(\"Digit {}: IOU: {} Dice Score: {}\".format(idx, iou, dice_score))\n",
        "  average_iou += iou\n",
        "\n",
        "grade = average_iou * 10\n",
        "\n",
        "print(\"\\nGrade is \" + str(grade))\n",
        "\n",
        "PASSING_GRADE = 60\n",
        "if (grade>PASSING_GRADE):\n",
        "  print(\"You passed!\")\n",
        "else:\n",
        "  print(\"You failed. Please check your model and re-train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvw0HLY2kV3w"
      },
      "source": [
        "## Save the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULCfGHEKkaO0"
      },
      "outputs": [],
      "source": [
        "# Save the model you just trained\n",
        "model.save(\"temp_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxpkHjH40s_F"
      },
      "outputs": [],
      "source": [
        "# Install packages for compatibility with the autograder\n",
        "\n",
        "# NOTE: You can safely ignore errors about version incompatibility of\n",
        "# Colab-bundled packages (e.g. xarray, pydantic, etc.)\n",
        "\n",
        "!pip install tensorflow==2.8.0 --quiet\n",
        "!pip install keras==2.8.0 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEcDAyaNMyW-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if you have the correct Tensorflow version\n",
        "assert tf.__version__ == '2.8.0', f'You have TF{tf.__version__}. Please install the grader-compatible Tensorflow and select Runtime > Restart Session'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljeWKuSKYEHE"
      },
      "outputs": [],
      "source": [
        "# Load the model you saved earlier\n",
        "model = tf.keras.models.load_model(\"temp_model.h5\", compile=False)\n",
        "\n",
        "# Save the model with the compatible TF version\n",
        "model.save(\"final_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCd50-pubX_o"
      },
      "outputs": [],
      "source": [
        "# You can also use this cell as a shortcut for downloading your model\n",
        "from google.colab import files\n",
        "files.download(\"final_model.h5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "56d44d6a8424451b5ce45d1ae0b0b7865dc60710e7f74571dd51dd80d7829ee9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
